atlas {
  environment {
    cf_port = ${?PORT}
  }

  core {
    model {
      step = 1s
    }

    db {
      class = "com.netflix.atlas.core.db.MemoryDatabase"

      // How often to rebuild the index for the memory database
      rebuild-frequency = 5s

      // 1h with 60s step
      block-size = 600

      // 6h of data overall
      num-blocks = 6
    }
  }

  webapi {
    tags {
      max-limit = 1000000
    }

    graph {
      // Change default start time on graph to smaller range more typical for testing
      start-time = e-30m
    }

    publish {
      // Max age for a datapoint. By default it is one step interval. If the timestamps are
      // normalized locally on the client 2 times the step is likely more appropriate.s
      max-age = 21600000
      // increase default key length
      rules = [
        {
          class = "com.netflix.atlas.core.validation.KeyLengthRule"
          min-length = 2
          max-length = 128
        },
        {
          class = "com.netflix.atlas.core.validation.MaxUserTagsRule"
          limit = 100
        }
      ]
    }

  }

  akka {
    atlascluster {
      number-of-shards = 4
      cluster = [
        {
          name = "publish"
          proxy = off
          class = "com.netflix.atlas.webapi.ClusteredPublishActor"
        },
        {
          name = "db"
          proxy = off
          class = "com.netflix.atlas.webapi.ClusteredDatabaseActor"
        }
      ]
    }


    #
    # Default config will load a LocalDatabaseActor and DeadLetterStatsActor
    # clear out the list and just add in DeadLetterStatsActor (may be able to drop it too)
    actors = []
    #actors = [
    #  {
    #    name = "deadLetterStats"
    #    class = "com.netflix.atlas.akka.DeadLetterStatsActor"
    #  }
    #]

    api-endpoints = [
      "com.netflix.atlas.webapi.PublishApi",
      "com.netflix.atlas.webapi.TagsApi",
      "com.netflix.atlas.webapi.RenderApi",
      "com.netflix.atlas.webapi.GraphApi",
      "com.netflix.atlas.akka.HealthcheckApi",
      "com.netflix.atlas.akka.ConfigApi",
      "com.netflix.atlas.akka.StaticPages"
    ]
    port = 7101
  }
}

# Eager initialize persistence
akka {
  extensions = [
    "akka.persistence.Persistence"
  ]

}

akka {
  actor {
      provider = "akka.cluster.ClusterActorRefProvider"
      #dispatcher = "kafka-journal.default-dispatcher"
      default-dispatcher {
        throughput = 5
        fork-join-executor {
          parallelism-factor = 16.0
        }
      }
      #default-dispatcher {
      #  type = Dispatcher
      #  executor = "fork-join-executor"
      #  fork-join-executor {
      #    parallelism-min = 2
      #    parallelism-max = 8
      #  }
      #}

      deployment {
        default {
          router = "round-robin-pool"
        }
      }
      default-mailbox {
        mailbox-type = "com.netflix.atlas.akka.UnboundedMeteredMailbox"
        path-pattern = ${atlas.akka.path-pattern}
      }
      cluster-mailbox {
        mailbox-type = "com.netflix.atlas.akka.ClusterMailbox"
        path-pattern = ${atlas.akka.path-pattern}
      }
      serializers {
        #kryo = "com.twitter.chill.akka.AkkaSerializer"
        kafka-event = "akka.persistence.kafka.journal.KafkaEventSerializer"
        kafka-snapshot = "akka.persistence.kafka.snapshot.KafkaSnapshotSerializer"
      }
      serialization-bindings {
        #"akka.persistence.kafka.Event" = kafka-event
        "akka.persistence.kafka.snapshot.KafkaSnapshot" = kafka-snapshot
        #"com.netflix.atlas.core.model.Block" = kryo
        #"com.netflix.atlas.core.model.CollectorStats" = kryo
        #"com.netflix.atlas.core.model.ConsolidationFunction" = kryo
        #"com.netflix.atlas.core.model.DataExpr" = kryo
        #"com.netflix.atlas.core.model.Datapoint" = kryo
        #"com.netflix.atlas.core.model.DataVocabulary" = kryo
        #"com.netflix.atlas.core.model.DefaultSettings" = kryo
        #"com.netflix.atlas.core.model.DsType" = kryo
        #"com.netflix.atlas.core.model.EvalContext" = kryo
        #"com.netflix.atlas.core.model.Expr" = kryo
        #"com.netflix.atlas.core.model.FilterExpr" = kryo
        #"com.netflix.atlas.core.model.FilterVocabulary" = kryo
        #"com.netflix.atlas.core.model.MathExpr" = kryo
        #"com.netflix.atlas.core.model.MathVocabulary" = kryo
        #"com.netflix.atlas.core.model.ModelExtractors" = kryo
        #"com.netflix.atlas.core.model.package" = kryo
        #"com.netflix.atlas.core.model.Query" = kryo
        #"com.netflix.atlas.core.model.QueryVocabulary" = kryo
        #"com.netflix.atlas.core.model.ResultSet" = kryo
        #"com.netflix.atlas.core.model.StatefulExpr" = kryo
        #"com.netflix.atlas.core.model.StatefulVocabulary" = kryo
        #"com.netflix.atlas.core.model.StyleExpr" = kryo
        #"com.netflix.atlas.core.model.StyleVocabulary" = kryo
        #"com.netflix.atlas.core.model.SummaryStats" = kryo
        #"com.netflix.atlas.core.model.Tag" = kryo
        #"com.netflix.atlas.core.model.TaggedItem" = kryo
        #"com.netflix.atlas.core.model.TagKey" = kryo
        #"com.netflix.atlas.core.model.TimeSeq" = kryo
        #"com.netflix.atlas.core.model.TimeSeries" = kryo
        #"com.netflix.atlas.core.model.TimeSeriesExpr" = kryo
        #"com.netflix.atlas.core.model.FunctionTimeSeq" = kryo
        #"com.netflix.atlas.core.model.ArrayTimeSeq" = kryo
        #"com.netflix.atlas.core.model.OffsetTimeSeq" = kryo
        #"com.netflix.atlas.core.model.MapStepTimeSeq" = kryo
        #"com.netflix.atlas.core.model.BinaryOpTimeSeq" = kryo
        #"com.netflix.atlas.core.model.UnaryOpTimeSeq" = kryo
        #"com.netflix.atlas.core.model.ConsolidationFunction" = kryo
        #"com.netflix.atlas.core.model.DsType" = kryo
        #"com.netflix.atlas.core.util.SmallHashMap" = kryo
        #"com.netflix.atlas.webapi.ClusteredDatabaseActor" = kryo
        #"com.netflix.atlas.webapi.ClusteredDatabaseActor$GetShardedData" = kryo
        #"com.netflix.atlas.webapi.ClusteredDatabaseActor$GetShardedTags" = kryo
        #"com.netflix.atlas.webapi.ClusteredDatabaseActor$GetShardedTagKeys" = kryo
        #"com.netflix.atlas.webapi.ClusteredDatabaseActor$GetShardedTagValues" = kryo
        #"com.netflix.atlas.webapi.ClusteredPublishActor$IngestTaggedItem" = kryo
        #"com.netflix.atlas.webapi.ClusteredPublishActor" = kryo
        #"com.netflix.atlas.webapi.ClusterPublishEvt" = kryo
        #"com.netflix.atlas.webapi.ClusterDatabaseEvt" = kryo
        "com.netflix.atlas.webapi.ClusteredPublishActor$IngestTaggedItem" = kafka-event
        "com.netflix.atlas.webapi.ClusteredPublishActor" = kafka-event
        "com.netflix.atlas.webapi.ClusterPublishEvt" = kafka-event
        "com.netflix.atlas.webapi.ClusterDatabaseEvt" = kafka-event
        #"com.netflix.atlas.webapi.DatabaseProvider" = kryo
        #"com.netflix.atlas.webapi.ExprApi" = kryo
        #"com.netflix.atlas.webapi.GraphApi" = kryo
        #"com.netflix.atlas.webapi.GraphRequestActor" = kryo
        #"com.netflix.atlas.webapi.GraphApi$DataResponse" = kryo
        #"com.netflix.atlas.webapi.PublishApi" = kryo
        #"com.netflix.atlas.webapi.RenderApi" = kryo
        #"com.netflix.atlas.webapi.TagsApi" = kryo
        #"com.netflix.atlas.webapi.TagsApi$ValueListResponse" = kryo
        #"com.netflix.atlas.webapi.TagsApi$KeyListResponse" = kryo
        #"com.netflix.atlas.webapi.TagsApi$TagListResponse" = kryo
        #"com.netflix.atlas.webapi.TagsRequestActor" = kryo
        #"com.netflix.atlas.akka.ClusterMailbox" = kryo
        #"akka.actor.ActorRef" = kryo
        #"spray.http.HttpResponse" = kryo
      }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 2552
    }
  }

  # See http://doc.akka.io/docs/akka/snapshot/scala/cluster-sharding.html
  cluster {
    #use-dispatcher = "kafka-journal.default-dispatcher"
    use-dispatcher = "akka.actor.default-dispatcher"
    sharding {
      #use-dispatcher = "kafka-journal.default-dispatcher"
      use-dispatcher = "akka.actor.default-dispatcher"

       rebalance-interval = 10 s
       # sharding cannot use the kafka journal, force it to leveldb
       journal-plugin-id = "akka.persistence.journal.leveldb"
       snapshot-plugin-id = "akka.persistence.snapshot-store.local"

       remember-entities = off
       # Setting for the default shard allocation strategy
       least-shard-allocation-strategy {
         # Threshold of how large the difference between most and least number of
         # allocated shards must be to begin the rebalancing.
         rebalance-threshold = 1

         # The number of ongoing rebalancing processes is limited to this number.
         max-simultaneous-rebalance = 1
      }
      entity-recovery-strategy = "all"
      coordinator-singleton = ${akka.cluster.singleton}

    }
    roles = [ "backend" ]
    seed-nodes = [
      "akka.tcp://atlas@127.0.0.1:2552"
      #"akka.tcp://atlas@127.0.0.1:2553"
    ]

    #auto-down-unreachable-after = 10s
  }
}



cluster-mailbox {
  mailbox-type = "com.netflix.atlas.akka.ClusterMailbox"
  path-pattern = ${atlas.akka.path-pattern}
}

#akka {
#  persistence {
#    journal {
#      #plugin = "akka.persistence.journal.leveldb"
#      plugin = "kafka-journal"
#    }
#    snaphost-store {
#      plugin = "kafka-snapshot-store"
#    }
#  }
#}

#akka.persistence.journal.plugin = "akka.persistence.journal.leveldb"
#akka.persistence.journal.leveldb.dir = "target/journal"
#akka.persistence.snapshot-store.plugin = "akka.persistence.snapshot-store.local"
#akka.persistence.snapshot-store.local.dir = "target/snapshots"

akka.persistence.journal.plugin = "kafka-journal"
akka.persistence.journal.auto-start-journals = [ "kafka-journal" ]
#akka.persistence.snapshot-store.plugin = "kafka-snapshot-store"
#akka.persistence.snapshot-store.auto-start-snapshot-stores = [ "kafka-snapshot-store" ]

kafka-journal {

  # FQCN of the Kafka journal plugin
  class = "akka.persistence.kafka.journal.KafkaJournal"

  # Dispatcher for the plugin actor
  plugin-dispatcher = "kafka-journal.default-dispatcher"
  #plugin-dispatcher = "akka.actor.default-dispatcher"

  # Number of concurrent writers (should be <= number of available threads in
  # dispatcher).
  write-concurrency = 8

  # The partition to use when publishing to and consuming from journal topics.
  partition = 0

  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }

  consumer {
    # -------------------------------------------------------------------
    # Simple consumer configuration (used for message replay and reading
    # metadata).
    #
    # See http://kafka.apache.org/documentation.html#consumerconfigs
    # See http://kafka.apache.org/documentation.html#simpleconsumerapi
    # -------------------------------------------------------------------

    socket.timeout.ms = 30000

    socket.receive.buffer.bytes = 65536

    fetch.message.max.bytes = 1048576
  }

  producer {
    # -------------------------------------------------------------------
    # PersistentRepr producer (to journal topics) configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    #
    # The metadata.broker.list property is set dynamically by the journal.
    # No need to set it here.
    # -------------------------------------------------------------------

    request.required.acks = 1

    # DO NOT CHANGE!
    producer.type = "sync"

    # DO NOT CHANGE!
    partitioner.class = "akka.persistence.kafka.StickyPartitioner"

    # DO NOT CHANGE!
    key.serializer.class = "kafka.serializer.StringEncoder"

    # Increase if hundreds of topics are created during initialization.
    message.send.max.retries = 5

    # Increase if hundreds of topics are created during initialization.
    retry.backoff.ms = 100

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  event.producer {
    # -------------------------------------------------------------------
    # Event producer (to user-defined topics) configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    # -------------------------------------------------------------------

    producer.type = "sync"

    request.required.acks = 0

    topic.mapper.class = "akka.persistence.kafka.DefaultEventTopicMapper"

    key.serializer.class = "kafka.serializer.StringEncoder"

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  zookeeper {
    # -------------------------------------------------------------------
    # Zookeeper client configuration
    # -------------------------------------------------------------------

    connect = "localhost:2181"

    session.timeout.ms = 6000

    connection.timeout.ms = 6000

    sync.time.ms = 2000
  }
}

kafka-snapshot-store {

  # FQCN of the Kafka snapshot store plugin
  class = "akka.persistence.kafka.snapshot.KafkaSnapshotStore"

  # Dispatcher for the plugin actor.
  plugin-dispatcher = "kafka-snapshot-store.default-dispatcher"
  #plugin-dispatcher = "akka.actor.default-dispatcher"
  # The partition to use when publishing to and consuming from snapshot topics.
  partition = 0

  # Topic name prefix (which prepended to persistenceId)
  prefix = "snapshot-"

  # If set to true snapshots with sequence numbers higher than the sequence number
  # of the latest entry in their corresponding journal topic are ignored. This is
  # necessary to recover from certain Kafka failure scenarios. Should only be set
  # to false for isolated snapshot store tests.
  ignore-orphan = true

  # Default dispatcher for plugin actor.
  default-dispatcher {
    type = Dispatcher
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 8
    }
  }

  consumer {
    # -------------------------------------------------------------------
    # Simple consumer configuration (used for loading snapshots and
    # reading metadata).
    #
    # See http://kafka.apache.org/documentation.html#consumerconfigs
    # See http://kafka.apache.org/documentation.html#simpleconsumerapi
    # -------------------------------------------------------------------

    socket.timeout.ms = 30000

    socket.receive.buffer.bytes = 65536

    fetch.message.max.bytes = 1048576
  }

  producer {
    # -------------------------------------------------------------------
    # Snapshot producer configuration.
    #
    # See http://kafka.apache.org/documentation.html#producerconfigs
    #
    # The metadata.broker.list property is set dynamically by the journal.
    # No need to set it here.
    # -------------------------------------------------------------------

    request.required.acks = 1

    producer.type = "sync"

    # DO NOT CHANGE!
    partitioner.class = "akka.persistence.kafka.StickyPartitioner"

    # DO NOT CHANGE!
    key.serializer.class = "kafka.serializer.StringEncoder"

    # Increase if hundreds of topics are created during initialization.
    message.send.max.retries = 5

    # Increase if hundreds of topics are created during initialization.
    retry.backoff.ms = 500

    # Add further Kafka producer settings here, if needed.
    # ...
  }

  zookeeper {
    # -------------------------------------------------------------------
    # Zookeeper client configuration
    # -------------------------------------------------------------------

    connect = "localhost:2181"

    session.timeout.ms = 6000

    connection.timeout.ms = 6000

    sync.time.ms = 2000
  }
}

test-server {
  # -------------------------------------------------------------------
  # Test Kafka and Zookeeper server configuration.
  #
  # See http://kafka.apache.org/documentation.html#brokerconfigs
  # -------------------------------------------------------------------

  zookeeper {

    port = 2181

    dir = "data/zookeeper"
  }

  kafka {

    broker.id = 1

    port = 6667

    num.partitions = 2

    log.cleanup.policy = "compact"

    log.dirs = data/kafka

    log.index.size.max.bytes = 1024
  }
}
